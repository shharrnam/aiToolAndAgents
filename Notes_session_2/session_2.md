# Session 2 Recording and Meeting Notes link

- Notes -> https://docs.google.com/document/d/11MJJAiMngitY8xa-XQByBPdLlZRcOwRKZ0npubJTqbI/edit?usp=sharing
- Recording -> https://drive.google.com/file/d/1zlsIMINlm-yqfnsNwunaIspO_fRl5K-7/view?usp=sharing


## Session 2 Notes
Details
Notes Length: Long
Addressing the Initial Technical Hiccup with the Event Link: Teacher OP began by addressing a major hiccup regarding the event link, questioning if anyone was unable to see it on the events page and if participants were joining via a link shared in the chat. Yatendra Singh Ranwat confirmed joining via the shared chat link. Teacher OP requested two minutes to coordinate, expressing uncertainty about why the event link was missing from the event page, noting that many non-members needed to be reached. Dipesh Sukhani checked the event page and confirmed the link was still not there, only seeing event details text relevant to testing, but "no link to the Google meet test" (00:00:00). Praneel Jain also confirmed there was no link. Yatendra Singh Ranwat asked if Teacher OP had updated this, while Teacher OP confirmed they had added the link again, stating it was previously added and was present, but Teacher OP could not check the event page themself since they were the host. Vaibhav Bansal mentioned they could only see the link in the event chat comments. Teacher OP suggested waiting a couple of minutes and doing a post, while Praneel Jain suggested adding the link to the "about section of the event itself" (00:11:40).
Managing Event Registration Numbers and Google Meet Limitations: Yatendra Singh Ranwat inquired about the number of people joining, stating that Google Meet has a limit of 100 users (00:11:40). Teacher OP clarified that there were 245 registrations, acknowledging the blunder of people not seeing the link. Teacher OP also stated that their account has a 500-person limit, which was not the problem. Since the page was not updating with the link, Teacher OP decided to wait five minutes and then start with the attendees present, regardless of the issue. Praneel Jain asked if the about page was also not updating. Teacher OP planned to try sending an email to people and asked the growth team to handle sending a mail to those who could not join, promising to redo the session for those unable to attend (00:12:47).
Session Context and Learning Approach for LLMs: Teacher OP addressed the attendees, asking how many were present for the last session. Teacher OP explained that the focus is on the very basic and simple concepts of how LLMs work. They reiterated that there is a slight, unavoidable learning curve, regardless of whether a participant uses no-code, code platforms, or an IDE like "lovable repleten". The speaker emphasized the need to understand certain logic and basics for proper LLM usage, noting that mastering any specific platform, such as ENM, could take four or five months (00:16:18). Teacher OP stressed that understanding the basics prevents being "bound by a platform" and avoids relearning everything from scratch if blocked on one platform. Teacher OP promised to make the current session easy, avoiding technical jargon, and shared that they are not a developer but have been learning for two and a half to three years, having distilled their learning approach so that anyone can master these concepts in six months, even when starting from scratch (00:17:55).
Accessing Session Materials via GitHub and README: Teacher OP detailed how attendees could access the session materials, including the first session's content (00:17:55). They provided a link to a GitHub repository, explaining that a GitHub repo is simply a link to a folder of files and should not be intimidating. The README file within the repo was described as "nothing but a notion doc for for developers" written in simple, practical language. The README contains the details of session one, including the learning curve based on current skill level, meeting notes, recordings, transcripts, Q&As, and sample code. Teacher OP explained that participants can clone the repo to start practicing after viewing the video (00:18:50). The speaker emphasized that reviewing the first session is important for understanding everything "in very very depth" and making the tools fun to play around with (00:20:00).
Introduction to the Integrated Development Environment (IDE) and Key Components: Teacher OP shifted to starting the session, providing a quick recap and moving into today's concepts. They introduced the IDE, speaking in "layman terms" and assuming no prior development knowledge. Teacher OP acknowledged the IDE looks overwhelming but stated that for beginners, only three things are important (00:20:00). The first important component is the Explorer, a button in every IDE that provides access to files, mirroring the structure seen on the GitHub repo, which allows viewing the code of different files. The second important component is the Terminal, which can be opened with a button or by using "control tilda". The terminal is used to "execute or run any file" (00:21:17). Teacher OP asserted that these are the only two things required to learn about the IDE for the next year if a person wants to learn, practice, or build automation, advising the audience to ignore everything else as "it's literally useless" (00:22:17).
Recap of First Session: Level One Fundamentals and the Core API Call: Teacher OP recapped the first session's focus on Level 1 and Level 2 fundamentals, which aimed to overcome the blocker of starting with LLMs from a basic level. They stated there is literally only one API call, with a similar structure across every LLM provider, and this is the only element needed to run the highlighted five lines of code. Practicing this code unblocks understanding of many other concepts because APIs for any other product work the same way. The power of this API lies in its ability to evolve into at least 100 different functionalities by changing a few parameters (00:22:17). Teacher OP demonstrated running the file named `step one` by activating a virtual environment (VENV) using a Python command, which was likened to having a necessary graphic card to run a game (00:23:43). The command used to run the file was `Python 3 -L step one` after navigating to the `fundamental level one` folder using the `cd` command (00:24:50).
Demonstration of the Simple API Call and its Evolution: Teacher OP showed the result of running `step one`, which was a simple API call sending the message "hello cloud" to Anthropic's endpoint and receiving the response "hello it's nice to meet you and how how can I help you today" (00:24:50). They explained that this single API call evolves into many things, such as chat, simulation, and agents, and understanding how LLMs function under the hood unblocks understanding of other platforms that use the same API. The speaker then demonstrated a slight improvement in the second step, using the same API but adding a system prompt. In this demonstration, the system prompt instructed the AI to act as "the founder of growth" and respond in "HTML formatted data," greeting people with the phrase "WhatsApp chat" (00:26:35).
Demonstration of Structured Output via System Prompt and Evolution to Tool Use: Teacher OP ran the next step, `step three`, which showed the AI responding in HTML formatting as requested by the system prompt, thus changing the AI's behavior (00:27:35). Teacher OP then introduced the concept of ensuring reliability for business automations by requiring a particular structure in the LLM's response. This prevents developers from writing extensive parsing edge cases and eliminates failures in business automations (00:28:45). This guaranteed accurate output is achieved through a parameter called "tools," which uses the same six lines of core API code. A tool was defined as a "structured prompt" called "the growth founder meets and greets people," with specific requirements for output: a string for the greeting and about GrowthX, and an array (list) for what GrowthX does, who should join, and why join (00:29:38).
Demonstration of Structured Output and Behavior Adaptation with Tools: Teacher OP explained that the tool structure allows for defining output formats, such as a list of bullet points or even a nested array for things like multiple-choice questions (00:29:38) (00:32:01). Furthermore, the description within the system prompt allows defining the AI's behavior, exemplified by instructing the AI to "always respond in a way you Yoda would respond". Running the script showed the raw API response using the tool (00:30:45). Since the tool parameters and schema were defined, the speaker knew what to expect, eliminating edge cases. The response showed the structured output—a string for the greeting, a string for about GrowthX, and arrays (lists) for the other items—while also adapting to the requested behavior (talking like Yoda) for the "why join growth x" list (00:32:01).
Real Business Case Overview Using LLM Tools for Assessment Analysis: Teacher OP provided a quick overview of a real business case utilizing tools, which was framed as a simple API call resulting in a response (00:33:05). The example involved a platform like Leadcode, where the business team wanted to expand their online assessment system to new technologies for AI and data engineers, requiring an assessment of a large list of skills (00:34:20). Product managers needed to determine if the current system was capable, what the standard assessment should be, and why they might not be able to perform it (00:35:50). This traditional process would take a product manager 6 to 9 months or require finding 100 highly skilled experts. Instead, they built a simple AI script to take the requirements, validate them, search the web, and respond with all the necessary structured data (00:36:48).
Deep Dive into the Business Case API Call and Structured Output Tool: The core of the business solution was the same six lines of API code, supplemented by a system prompt and a tool (00:36:48). The system prompt established the AI as an expert with knowledge of Leadcode's current system and assessment capabilities, with variables that change for each skill analyzed in the list (00:38:06). To ensure structured data, a tool was defined to analyze the skill and return results in a specific format, including discipline name, skill name, whether a technical assessment is required, if it can be assessed in the current setup, assessment names if possible, and infrastructure requirements if not. This script allowed product managers to obtain factually correct, citation-backed data in a couple of days (00:38:52). Teacher OP demonstrated navigating to the Level 2 folder and running `step five`, which assessed the skills and returned a CSV file containing the data in the required format (00:40:07).
Transition to Chat Simulations and Agents (Level Three Concepts): Teacher OP summarized the first session, which took two hours, and transitioned to today's session, focusing on how messaging evolves into chat simulations and how those simulations become agents, tools, or workflows. Teacher OP explained that they would define what an "actual agent" becomes (00:42:12). The speaker then navigated to the third folder. Teacher OP previewed the end goal of the session: a simple automation that writes and publishes blogs with market research and citations on a real website within three minutes, and a presentation agent that builds a presentation within three to four minutes (00:44:04).
Evolving the API Call into Message Simulation and Context Building: Teacher OP moved on to the concept of chat, where the single API call evolves into a simulation of messages. A message simulation must follow a defined format: an assistant message is followed by a user message, and so on. This sequence is critical for building "memory or context". The core API call now includes a system message and a list of messages. The user input is the starting point of the list, and the assistant's response is appended to the list, which grows and forms a chat (00:45:19). This chat simulation is not just for chatbots; it can be used with tools and third-party inputs to build agents and tools (00:46:56).
Demonstration of Chat Simulation and Memory (Context): Teacher OP ran the chat demonstration, which started by providing instructions on using the terminal (typing `exit` to quit, `clear` to start a new conversation). The AI, programmed to speak like Yoda as the founder of GrowthX, responded to "hi" (00:46:56). Teacher OP showed that the system was printing the context, which grew from the user's first message to the assistant's response, and so on. To test the memory, Teacher OP provided a random fact: "I scored 55% in 12th" (00:48:21). The context grew to six messages, and the AI responded to the question about the user's 12th percentage by correctly recalling "55%". Teacher OP explained that this memory (context) is built by sending the history of messages (the growing list) with every new API call (00:49:53).
Storing Conversation History and Resuming Chats: Teacher OP then introduced the concept of storing the conversation history for future access or resuming a chat, moving beyond just local memory (00:49:53). This storage, while demonstrated locally in a simple file, could be in a database or CSV. The core API call remained the same, emphasizing that this one API call is the foundation for solving hundreds of problems (00:50:56). Running the second chat, the system assigned an ID to each chat and stored the conversations in a local folder called `conversations`, visible to the user (00:52:15). This demonstrated that the growing list of messages (the context) is what is actually being sent in the API call (00:53:11).
Managing Context Limits and Message Sequence Logic: Teacher OP addressed the issue of context limits, which necessitate compacting or saving conversations, noting that the limit is typically 200k to 400k tokens (800,000 to 1 million words, or about 500 blogs). The most crucial aspect of this chat logic is maintaining the sequence: "Every user message has to be followed by an assistant message. Every assistant message then has to be followed by a user message," as any break in this logic will cause the system to break (00:54:04). Teacher OP demonstrated this by running the second chat again after quitting, and the system recognized the saved conversations, allowing the user to resume the old chat (00:55:08). The AI correctly remembered a previous detail ("I'm from Japur") from the old conversation (00:56:28).
Introducing the Most Important Concept: Tool Use for External Access: Teacher OP introduced the final and most important concept, asserting that mastering the concepts up to chat three allows a person to solve "any problem that you want to solve from an ID" within seven days. They emphasized that they built the UI and the last two folders themselves in four hours by utilizing their understanding of these core concepts. They reiterated that one does not have to write code but simply navigate, identify bugs, and tell the LLM (like "cloud") to solve them (00:57:58). This is considered "white coding" at a foundational level, layer one, below platforms like N10 (layer two) or Replit/Lovable (layer three) (00:59:21).
Demonstration of Tool Use for Real-Time Data (Weather Tool): Teacher OP ran the final script, `chat 3`, which uses the same six lines of code but introduces a tool called the "weather tool" (01:00:15). This tool serves two purposes: getting structured output (as demonstrated earlier) and parsing output to call any third-party API. When LLMs like GPT or Cloud seem to go into a research mode, they are telling the developer they need to access a tool, as the LLM cannot access the internet directly (01:01:19). Running the chat, Teacher OP started a new conversation and asked for the weather in Bangalore. Since LLMs are only trained until a certain date, accessing real-time weather requires talking to the internet via the tool. The AI responded with the current temperature, humidity, and wind speed for Bangalore, which can be validated as real-time data (01:02:35).
Dissecting the Tool Execution and Tool Result Messages: Teacher OP explained the crucial mechanism behind the tool use by reviewing the conversation history in the file. When the user asked for the weather in Bangalore, the assistant responded by stating it wanted to use a tool—the `get weather` tool—for the city Bangalore and country India (01:04:08). This is the cue for the developer to perform a "tool execution". The execution involves a script (e.g., Python) that calls a third-party API (Visual Crossing in this example) using the parameters requested by the LLM (01:05:12). The result of that API call is then returned to the LLM. The critical sequence rule mandates that an assistant message with a tool use must be followed by a user message called "tool result," which contains the API response data (e.g., temperature, humidity) (01:06:03). Finally, the assistant uses this tool result to generate a "human facing message" (a human-readable response) that is presented to the user (01:06:53).
Conclusion on Learning LLM APIs and Transition to Automations: Teacher OP concluded that the difference between a normal message and a tool-related message is the user response being in a `tool result` format. The speaker asserted that learning these four simple API calls (and three others, totaling seven) is the maximum requirement for solving 90% of problems and only requires 1 hour a day for 7 to 10 days. Following this, the session was to move on to actual UI-based automations, starting with a question and answer period (01:07:49).
Discussion of Real-Time Tools and the Q&A Section: Teacher OP began by mentioning simple tools like real-time blogs and real-time presentation building, noting that they would not cover complex tools in the current session as complex tools would require around an hour and a half to explain per tool. They mentioned that they had selected very simple tools based on input from people, and more complex tools would be covered in the next session. Teacher OP then directed attention to the Q&A section, encouraging attendees to upvote questions that resonated with them to quickly solve popular queries (01:09:04).
Explanation of Tool Result Generation (Responding to Anchula's question): Teacher OP addressed Anchula’s question about how the content of a user tool result is generated. Teacher OP explained that when the LLM decides to use a tool, they implemented a check to see if the "stop reason" is "tool use" and if the tool name is "get weather" within the tool use (01:09:04). If these conditions are met, a script called "execute weather tool" is run, which returns specific data like weather info, summary, temperature, humidity, and wind speed. This returned data is then used to manually generate the tool result, as the LLM cannot talk to the internet directly; actual executions must be written to make this happen. The message is appended with the type "tool result" and the formatted content (01:10:34). Teacher OP showed that the content of the tool result for "get weather for Bangalore" included the weather, summary, temperature, humidity, and wind speed, which was appended to the chat history. All these messages (assistant message, user message, and tool result) are sent together to the LLM provider to maintain sequence, though the Teacher OP noted that context reduction or token usage optimization was not a priority for them at this stage (01:11:47).
Explanation of the Temperature Parameter (Responding to a Patient's question): Teacher OP responded to a question about the objective of the temperature parameter, explaining that it controls the probability of hallucination in an LLM. Higher temperatures, such as 1 or 1.2, increase the probability of hallucination, causing the LLM to predict data it thinks might be probable, even if the value is not present. Reducing the temperature forces the model to only respond when the probability is very high (e.g., 90% probability that a word is the next one). Teacher OP recommended reviewing the first session for more detailed information, as this topic was discussed for about 20 minutes (01:12:50).
Clarification on Message Sequencing and Configuration (Responding to manish patkar's question): manish patkar requested clarification on the sequencing of messages, specifically asking how the roles (user/assistant) and the structure, including the tool block, are defined and configured. Teacher OP confirmed that they configured the sequencing. The API call to Anthropic includes the model name, system prompt, and messages. Initially, a single message is sent in the API call, but at the chat level, a list of messages is sent. This list, defined by the variable `messages`, builds by adding the user input in the format of `role: user, content: user input` (01:14:08). After an API call and an assistant response, a configuration reads the assistant message and appends it to the list in the format `role: assistant, content: assistant content` (01:15:20). This process of appending messages (user input, assistant response, subsequent user input, subsequent assistant response, etc.) continuously builds the list of messages for the conversation history. When a tool use message comes in, it is appended with a user message because the tool use call was an assistant message requesting tool execution, and the subsequent user message contains the tool result data built from the weather API (01:16:20).
Cost Implications of Sending the Messages List (Responding to Prem Praash's question): Teacher OP confirmed that sending the list of messages in the prompt (the complete conversation) increases the costing because the list of messages grows longer with every API call. The cost is determined by tokens: an initial API call might have 100 tokens and receive a 50-token response (total 150 tokens), and the next API call includes the previous 150 tokens plus the new input (e.g., 100 tokens, total 250 plus response). While context length can increase significantly (up to 200,000 tokens, or 1 million in beta), there are two distinct limits: an input limit (context) and an output limit (response length). The current maximum output token limit for Anthropic is 64,000 tokens (01:17:09) (01:22:26).
Best Practices for Tool Structure (Responding to Amarj's question): Teacher OP stated that the best practices for structuring a tool completely depend on the use case. They mentioned a simple one-page document by Cloud that advises having very clear tool descriptions detailing the tool's purpose. Tools should define required parameters to ensure the LLM always responds with that data (01:18:03). Teacher OP demonstrated their weather tool definition, which is English written in JSON format and passed as a prompt to the LLM. The tool is named "get weather" with a description, and the required parameters are `city name` and `country`. These parameters are necessary for the underlying weather API call to execute properly, and the tool definition changes based on how the tool is executed (01:19:06).
Model and Max Token Configuration: Teacher OP addressed a question about where the model name and token limits are specified, pointing out that in the code, the model name (e.g., `sonnet 4.5`) is explicitly mentioned and can be changed to other models like `haiku` or `opus`. The `max token` parameter controls the maximum length of the response, currently set to 1,000 tokens in the example, which can be increased (e.g., to 16,000 for long form blogs) (01:20:23). Teacher OP explained that setting a low limit, such as 150 tokens for generating tweets, limits the response length and prevents the LLM from trying to utilize more tokens. Teacher OP confirmed to Vaibhav Bansal that the `max token` is a hard limit (01:21:25).
Input vs. Output Token Limits and Costs: Teacher OP clarified the difference between input and output tokens. Input tokens are what is sent to the model (the total context, including the list of messages), and output tokens are the model's response. The cost for these two token types is different, with input tokens being cheaper and output tokens being expensive. The limit for input tokens (context) is up to 1 million, while the maximum output limit is 64,000 tokens (01:22:26).
Difference Between Inline and External Message Storage (Responding to V A's question): Teacher OP explained that when chatting "in line," the message list is built in real time and disappears when the session is closed. Storing messages externally (in different files or places with IIDs) is necessary to build automations where data is needed in subsequent steps. External storage is essential for building applications like a support chatbot, where refreshing the page would otherwise cause the loss of the last few messages (01:23:21).
Availability and Types of Tools (Responding to Manish's question): Teacher OP detailed two types of tools. The first type is user-executed tools, which are unlimited, as any platform with an available API can have a tool built for it (e.g., sending Slack messages, Stripe payments, or turning on smart lights). The second type is common server-level tools provided by LLM providers like OpenAI or Anthropic, such as web search. For server-level tools, the user does not need to write the execution or handle messages, as the provider performs the function on their server and returns the result as a user message (01:24:35). Teacher OP mentioned that a list of these server-level tools (like creating/reading files, managing memory, searching the internet, or executing code) is available in the documentation under session one’s additional learning resources (01:25:36).
Tools and Agents: Teacher OP confirmed that a collection of tools and the sequence in which they are used is what people call an agent (01:25:36). They noted that while complexities exist, they would not deep-dive into the extreme levels but would move one level up in the current session. Regarding the number of tools an agent can handle, Anthropic allows approximately 200 tools, with an accuracy of tool identification around 94% for Sonnet and Opus models, though they felt most use cases would require no more than 10 to 15 tools (01:26:49).
Tool Parameters and Input Tokens: Teacher OP clarified that tools are counted as input tokens because they are defined under tool parameters which are part of the prompt sent to the LLM. Any content in the system prompt, tool parameters, or the user/assistant message list contributes to the input tokens. The default input token limit is 200,000, but a beta mode parameter can increase it to 1 million, albeit at a current 2.5x increase in API cost across platforms (01:28:00).
Transition to Automated Blog and Presentation Builder: Teacher OP called for a quick 5-minute break before moving on to working with an automated blog and presentation builder. They described this next tool as complex enough, but one that they use for production on personal websites, mentioning it could still be improved by 10–15% (01:29:03).
Starting the Next Section and Repository Update: After the break, Teacher OP did a quick check to ensure attendees were present before proceeding to the next level of material, referring to the updated repository. They stated they would update the meeting notes and videos post-session, within three hours (01:34:07).
Moving to Real Agents and Workflow Automation: Teacher OP announced the shift to real agents, demonstrating one agent with a single workflow and then an agent that calls another agent. They noted that while the materials included subfolders for UI-based examples, in most cases (especially for internal workflow automation), a UI is unnecessary unless building a consumer-facing application (01:37:36). The UI examples for the blog and presentation tools share the same logic but require extra time for building the UI and response rendering. Teacher OP emphasized that they personally use no-code methods but understanding the underlying logic is crucial (01:38:36).
The Blog Creation Script and Key API Parameters: Teacher OP detailed the blog creation bot, which is a single script of about 700 lines (01:38:36). They highlighted the core API call containing six controlling lines: model definition, max token limit (set to 16,000 for long-form blogs), system prompt, messages list, and tools. A new parameter, `tool choice`, was introduced, useful for ensuring the assistant always uses tools rather than responding with text, which can interrupt automations (01:39:39).
Tools Integrated into the Blog Automation Agent: The agent uses several tools, including the server-level tool `web search`, which requires only the query and is handled by Anthropic (01:40:29). Custom tools include:
`image generator`: Called when the LLM needs to generate an image (e.g., a header image), configured to use Google’s latest image model, requiring a prompt (01:41:24).
`image uploader`: Needed because images generated by the image model cannot be directly accessed by web services like Shopify or WordPress (01:41:24). This tool uploads the image (currently configured for Superbase/Postgres DB) to get a publicly accessible URL (01:42:23). It requires the image's path and file name for execution (01:43:10).
`blog creator`: The core blog writer function, designed for structured output to ensure data is parsed and stored consistently, supporting templates and SEO guidelines (01:43:10). The output structure is currently CSV but could be HTML or Markdown. It defines properties like title, slug, meta title, meta description (max 100 characters for SEO), HTML content, summary, featured image link (from the uploader), category, tags, and author (01:44:01).
`blog insertter`: The final tool for executing the push of the blog to its required destination (Superbase, WordPress, Shopify, etc.), allowing the insertion execution to be dynamic (01:44:50).
Dynamic System Prompts and Research Logic: Teacher OP explained that the system has two prompts based on user input: one if the user provides a topic, and another if they do not. If no topic is provided, the system is instructed to first find existing blogs on the user's current website to prevent duplicate content, then perform a web search for trends, and research competitors (01:45:45). Brand context is inserted from a text file, allowing easy switching between brands. The system also passes instructions and existing blog titles to the LLM (01:46:39).
Tool Execution and Demonstration Setup: The execution logic for tools like image generation and image uploading is defined in separate methods. Teacher OP demonstrated running the script, correcting an initial error by activating the virtual environment (`benv`) (01:47:51). Teacher OP executed the script, providing the topic "title is one of my competitors. So my brand is reply. Title is a Reddit marketing tool," aiming to generate an alternative-focused blog (01:49:09).
Real-Time Agent Execution and Blog Output: The terminal logs showed the agent started research, executed the `image generator` tool, and saved an image locally, which was then inspected to confirm it was relevant (Title vs. Reply ID for a Reddit product) (01:50:43). The agent then used the `image uploader` to get a public URL for the image. The Teacher OP noted that generating one blog takes about 2.5 to 4 minutes, but the quality is high and data is factual (01:51:45). The resulting blog was published on a live domain, featuring the generated image, adhering to SEO guidelines, including all necessary headings, links, and schema markup (01:53:07). Crucially, the blog included references and citations, verifying that the agent searched the web for data (01:54:34).
Transition to UI-Based Agent (Blog Creator): Teacher OP transitioned to the UI-based version of the blog creator, noting that the single script is now distributed across multiple files (front end, tools, app, configuration) because running a UI requires a server (like a simple Python-based Flask app with Gunicorn) and API calls for front-end to back-end interaction (01:55:45). They demonstrated updating the brand context file in real-time through the UI, which updates the data on the server side. The UI allows the user to either provide a topic or click "generate" to autogenerate (01:56:55).
UI Agent Execution and Engagement: When `generate` was clicked without a topic, the UI showed logs confirming the autogeneration process: "autogenerating topic based on existing ideas and trends starting blog generation selecting a unique idea". Teacher OP noted the importance of notifications for user engagement (01:57:47). They mentioned potential improvements, such as adding tools for internal linking, tables, and multiple images. Teacher OP highlighted that they personally built this tool in 2–3 hours to avoid paying expensive monthly fees for similar services like Outrank (01:58:58).
Architecture and Scaling Considerations: Teacher OP acknowledged that while the logic and back-end are functional, complexity will arise with scaling (e.g., handling 10,000 users, rate limits, databases, payment gateways). They advised not worrying about these architectural problems unless they are actually encountered (02:00:08).
Final Blog Preview and Validation: The UI-generated blog was inserted successfully, and the preview showed the autogenerated topic: "multi subreddit marketing strategy... without getting banned" (02:01:03). The resulting blog maintained all structural and SEO requirements (H1s, H2s, links, schemas, and images). The agent performed at least 23 web searches to validate and source the data, confirmed by the presence of real, cited links in the references section (02:02:00).
Trusting Output and Verification Against Hallucination: Teacher OP addressed how to trust the factual nature of the content and avoid hallucinations. They rely on strict system prompts requiring web search and citation insertion, and keeping the temperature parameter low (02:02:50). They also suggested that for high efficiency, an additional parallel agent could be used solely for validation, checking if the information (e.g., pricing, features) is factually correct against provided data (02:03:58).
Parallel Tool Calls and Session Limitations: Teacher OP acknowledged a question about how to call tools in parallel to improve latency but declined to demonstrate it in the current session to avoid overwhelming attendees who may be new to the concepts. They noted that parallel calls are possible and they have existing products that utilize them for rapid database querying (02:04:41).
Simple Presentation Builder (First Principle Thought): Teacher OP introduced the simple presentation builder, built on the "first principle thought" that since AI is adept at creating web pages, it should create web pages and convert those into a presentation format, rather than having the LLM use PowerPoint or Google Slides. This basic version, once improved with a few more tools, could generate high-quality, custom presentations that otherwise cost $1,000–$1,500 from design studios (02:06:06).
Structure of the Presentation Builder Agent: The presentation builder is an app running on a server (local system) and uses two parallel agents: a main chat agent and a presentation builder agent, modularized for complexity and future expansion (02:07:04). Teacher OP demonstrated building a pitch deck for a Series A fundraise, allowing for the upload of brand assets (logos, screenshots) for reference on styling and colors. The agent is configured to ask factual questions (e.g., amount of fundraise, use of funds, team/market data) to ensure the output is accurate and requires minimal editing (02:07:57).
Demonstration of a Two-Agent Presentation Generation Workflow: Teacher OP demonstrated a simulation using a chat interface, with a fixed temperature of zero ensuring 98% consistent responsiveness, only a 2% variation. The initial task was a funding request for $2 million, and Teacher OP provided half the information, asking the main agent to search the web for details like revenue projections, team information, founder, co-founder, and previous fundraising efforts (02:08:51). The presentation generation process involved a main chat agent (the first simulation) that utilized a tool called "generate PPT," which is itself another AI simulation or agent. This second agent is invoked to handle context-heavy tasks, queue tasks in the background, and prevent the main chat from being blocked (02:11:19). This sub-agent was given the required data and had tools to create folders, write files based on brand guidelines, convert files into web-based screens, take screenshots, stitch them, and convert the final output into the required PPTX format for the user (02:12:06).
Agent Research and Output Generation Process: The process included the main agent thinking and gathering data, which was logged in the backend and visible as a list of messages. The main agent utilized the "generate PPT" tool (02:09:53). The presentation creation sub-agent used a tool to check the list of files and then another tool to open a headless browser, take screenshots of the generated HTML pages, and stitch them into a presentation format, as the user required a presentation and not raw HTML. The presentation was completed, and the sub-agent's task ended, leading to a human-readable message from the main chat with a download link for the completed PPT (02:13:22). The resulting eight-page presentation contained data provided by Teacher OP, such as current team size and revenue streams, as well as data gathered by the agent through web searches, including growth examples (Apple, Microsoft, Google, Netflix), details about offline events, pre-seed funding data, investors, and team vision/profiles (02:14:30).
Structure of the Two Agents and Their Tools: Teacher OP detailed the code structure, which included a protocol agent housing the main chat agent and the presentation agent, along with a common tool executor. The main AI agent was configured with two specific tools: a web search tool for data gathering and a "generate PPT" tool to call the second AI agent (02:15:49). The `generate PPT` tool required properties such as the topic, description, data details, brand logo/guideline/color details, which could be expanded to include brand assets and iconography. The second agent, the actual PPT creator agent, had a collection of tools for creating folders, creating files, reading files (useful for adding an edit/update functionality based on user input), updating files, listing files, and a crucial `return result` tool (02:16:43). The `return result` tool signals to the main agent that the sub-agent's work is complete, allowing the main agent to present the final output to the user (02:17:35).
Evolution of Tools and Agent Complexity: Teacher OP explained the evolution from a tool being a single API call to a tool becoming a sequence of API calls or even an entire AI agent simulation. This modular approach allows for building complex workflows, where specialized sub-agents can be assigned tasks like finding financial data, sourcing images, deciding fonts, or generating SVGs for bar charts and graphs, enabling the creation of high-quality presentations (02:18:34). Teacher OP suggested that while top human talent would outperform AI, the AI can achieve results similar to the average 90-95% of design agencies (02:19:48).
Future Session Topics and Initial Development Experience: Teacher OP indicated that based on attendee feedback, the next session could cover complex structures of two automations, a more complex blog agent, or a complex presentation agent, potentially solving a real business problem. By default, the plan included developing a support complex code bot and a website builder like "lovable," with an estimated build time of 2-3 days for 80-85% completion (02:19:48). In response to a question about the build time, Teacher OP stated the current extension builder chat tool took approximately 9 hours to build, facilitated by their 2.5 years of experience and focusing on learning a single API (02:21:01). Teacher OP explained that focusing on one API simplifies understanding other concepts like data models, data structures, rate limits, and common application types (CRUD operations) (02:22:28).
Preference for Python and Guidance for Beginners: Teacher OP mentioned starting development in NodeJS but found it confusing for beginners due to the blurred lines between frontend and backend files (TS, JSX, JS). They transitioned to Python with Flask and Jinja, which offered a clear separation between the Python backend and HTML frontend (02:22:28). While now using React for dynamic frontend data loading alongside a Python backend, Teacher OP recommended Python for beginners due to its simplicity, while experienced developers could use NodeJS, Rust, or Golang (02:23:49).
Handling Bot Status Updates and Server-Side Events: In response to a question regarding how "bot is thinking" status updates are shown, Teacher OP clarified that the response is received via API, and displaying it in the frontend requires configuring WebSockets or server-side events (SSE) to continuously update the user on the status of every task. The necessity of using SSE or WebSockets depends on the use case; it might be omitted for internal tools but is crucial for customer-facing tools to prevent user anxiety (02:24:50).
Experience with Multi-Tool Use Cases (MCPs) in Cloud Environments: Teacher OP shared negative early experiences with Multi-Tool Use Cases (MCPs) in cloud environments, noting that often 124 available tools are loaded even when only one, like keyword search, is needed (02:24:50). MCPs sometimes call multiple tools in parallel, providing random, unrequested data, and integrating a single AFS API took 10 minutes. Teacher OP concluded that MCPs are not yet efficient because they lack configurability and standardization across companies, necessitating custom tool executions alongside MCPs (02:25:59). The vision for an AI-first world is that companies like Stripe would consolidate 30 APIs into one endpoint for the LLM to call, streamlining raw data processing and reducing the need for developers to manage multiple APIs (02:26:50).
Resources and Advice for New Participants: Aaradhya Rai, a first-time participant who lacked an understanding of APIs, asked for resources (02:29:03). Teacher OP recommended starting with the provided GitHub repository, as it begins with single-API calls. They also suggested using resources directly from OpenAI or Cloud because most courses are based on their documentation. Teacher OP referred to "cookbooks" and a session one folder containing additional learning resources, noting that the link to Cloud's resources might need updating due to changes (02:30:32). Teacher OP defined "agent" as a simulation of a chat, or a more complex simulation of a chat (02:31:28).

