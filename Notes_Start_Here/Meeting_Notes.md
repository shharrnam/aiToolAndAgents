Nov 2, 2025
Session 1: A Practical Guide to Building AI Tools & Agents
Attachments Session 1: A Practical Guide to Building AI Tools & Agents 
Meeting records Transcript Recording 

Summary
Neel Seth,, initiated the meeting and tailored the session on AI LLM tools based on participant experience, recommending Cursor or VS Code over no-code tools like Replit for better debugging, and highlighting Cloud Code as a worthwhile investment despite having spent over three lakh rupees on API keys and LLMs over three years. Neil demonstrated the fundamental steps of interacting with an LLM via API calls, beginning with a simple Python script, evolving it with system prompts to define persona and format, and then introducing the "tools" parameter to ensure consistently structured, accurate output crucial for business automation. The session also covered a real-world use case of building an automated skill assessment platform using a structured script with tools like \`return analysis event\\` and \`web search\\` to enforce output requirements, improve accuracy, and allow for data analysis and subsequent transitions into chat-based agent development.

Details
Notes Length: Standard
Meeting Commencement and Attendance Neel Seth initiated the meeting, stating they would wait for attendees to join, anticipating around 180 people, and planned to start by 11:10 AM at the latest. They confirmed their audio clarity and instructed participants on how to use the Q&A feature within Google Meets for questions. Neel Seth noted a shortfall of approximately 110 people but decided to wait five more minutes (00:00:00).
Attendee Background and Experience While waiting for others to join, Neel Seth encouraged attendees to share their experience with coding, Integrated Development Environments (IDEs), AI tools, or cloud code in the chat, acknowledging the diverse backgrounds of participants including product managers, CEOs, developers, and marketers. Neel Seth sought this information to tailor the session's approach (00:00:00).
IDE Preference and Rationale Addressing a question from Robin Singhi, Neel Seth explained their preference for Cursor or VS Code over no-code tools like Replit, primarily because Replit can lead to blockers with no clear path for resolution (00:00:00). They further noted that Replit and similar platforms are essentially wrappers over LLM APIs, and understanding the fundamental knowledge provided by tools like VS Code or Cloud Code makes it easier to switch between tools and debug problems (00:05:00). Neel Seth mentioned using Cloud Code for the past eight months, finding it superior to Cursor, even though they are not professionally a developer but rather a product manager who has been learning to code (00:07:43).
Investment in AI Tools and ROI Neel Seth addressed a question from Kirin regarding recommended tools without excessive cost, stating they happily paid $200 for Cloud Code's max plan, considering it the best ROI. They shared that they have spent over three lakh rupees on API keys and fine-tuning LLMs over the past three years but only started seeing an extremely good ROI in the last year or year and a half. Neel Seth indicated they would not comment on the issue of hiring developers at the moment but acknowledged it as a challenge (00:11:20).
Session Agenda and Resources The planned session agenda included a 10-minute background segment, followed by 25 to 30 minutes of logically understanding concepts with simple, practical examples, and then moving to building or working toward a more complex agent. Neel Seth assured participants that all code, repositories, notes, and meeting recordings would be shared, and encouraged attendees to use the Q&A section for questions (00:12:23).
Neel Seth's Professional Background and AI Journey Neel Seth, introduced themself as a Growthx member for three years, with a background in technical product management and business monetization, despite being a computer science engineer who never formally practiced development. Their journey into LLMs began around 2022 at their last job at Dukaan, where they first used the GPT-3 text completion API for generating product descriptions (00:14:01). The launch of GPT 3.5 and its chat-based format marked a significant shift, leading to the development of autonomous agents using the open-source 'autogen' repository (00:15:16).
Evolution of AI Development and Personal Projects Neel Seth recounted how developers, including those on the 'autogen' project, built frameworks for conversational agents and plugins before they were natively supported by services like OpenAI, which later adopted and funded the approach (00:16:20). They also shared their experience building an internal system at Dukaan using the autogen repo to manage support tickets, which evolved into the launch of the successful SAS platform "botn" (00:17:19). Neel Seth left Dukaan in May 2024 to pursue building their own products and freelancing, motivated by the rapid evolution of models and tools (00:19:59).
Core Learning Philosophy and Effort Required Neel Seth set the expectation that the session would start from a very basic, layman's perspective, assuming no prior knowledge (00:22:14). They emphasized honesty regarding the learning curve, suggesting that mastering AI LLM tools does not require deep tech knowledge like building models, but rather learning to use existing products effectively to solve real business problems (00:23:29). Neel Seth claimed that because advanced systems like Stripe or Shopify have numerous APIs, the complexity is high, but with LLMs, there is fundamentally only one API (chat/message) that is capable of almost anything, making the learning process more streamlined. They asserted that spending about three days wrestling with an IDE and code is enough to understand the power of LLMs (00:25:03).
Practical Demonstration: The Fundamental API Call Neel Seth transitioned to a practical demonstration within VS Code, focusing on the single, fundamental API call required to interact with an LLM (00:35:20). They outlined the prerequisites: downloading an IDE (VS Code), installing Python, and obtaining an API key from a provider like Anthropic (00:37:39). Neel Seth demonstrated how to structure a Python script to load the Anthropic API key from an environment file and send a simple "hello claude" message (00:38:29). Running the script involved using the terminal to install the necessary Anthropic dependency and executing the Python file, illustrating the raw response received and how to extract the relevant text component (00:40:18).
Adding System Prompts (Step 2) Neel Seth showed how to evolve the API call by adding parameters, specifically a system message, which provides instructions or context for the model's response (00:45:43) (00:47:50). When asked by Viola Lewis to zoom in, Neel Seth did so, improving visibility (00:46:38). By setting the system prompt to define a persona (Udian, founder of Growthix) and a greeting, the LLM’s response changed from a generic "Hello" to "What's up champ, great to meet you I am Udian" (00:49:01).
The Role of Temperature in Model Responses Neel Seth addressed Shraman’s question about the temperature variable, explaining that while high temperatures (closer to 1) once led to random text, newer models are more controlled, though high temperature still increases the probability of less factual or more "hallucinated" responses, especially in complex or lengthy tasks (00:52:07). Temperature, as a parameter in the API call, fundamentally defines the probability of the next predicted token being outputted (00:54:35). Neel Seth clarified that models are highly knowledgeable but still fundamentally predict the next token based on probability, which limits their capabilities compared to actual human thought (00:56:51).
Evolving API with System Prompt Formatting Neel Seth discussed evolving an existing API by modifying the system prompt to enforce a specific output format, such as HTML formatted data, instead of relying on general text (00:59:45). They noted that using system prompts for formatting is common but can be inefficient, contrasting it with formats like JSON or Markdown, which have heavy training data and are suitable for development and content creation (01:01:00). Neel Seth demonstrated that requesting HTML formatted data in the system prompt allows for immediate use in contexts like blog writing, circumventing manual formatting efforts (01:02:20).
Limitations of Format Prompting Neel Seth explained that directly prompting the model for a specific format (e.g., HTML, JSON) in the system or user message is no longer the best practice, as it can lead to inaccuracies and errors in automation due to the model sometimes adding unexpected strings or text alongside the requested format (01:03:08). They noted that LLM providers have introduced new parameters to solve this specific use case, which unlocks significant business automation value (01:04:08).
System Prompt vs. User Message Prompting Addressing a question from Anchul, Neel Seth clarified that the choice between using the system attribute or a user message for prompting depends on the desired automation or business value (01:05:28). While historically people added prompts in user messages due to system message length limitations, Neel Seth stated that for formatting or changing behavior, using the system prompt or the dedicated tool parameter is now generally sufficient, as they have not seen practical differences in accuracy favoring the user message (01:06:54). Neel Seth emphasized following the API provider's documentation for optimal results (01:07:43).
Introduction of Tools Parameter for Structured Output Neel Seth introduced the "tools" parameter (formerly called "functions") as the modern solution for ensuring 100% accurate and consistently structured model responses, which is critical for business value and reliable integrations. They explained that tools define the expected structure, or schema, of the output, preventing issues like syntax errors that were previously addressed through lengthy, sometimes amusing, demands in the prompt (01:08:59).
Defining Tool Structure and Schema Neel Seth detailed how to define a tool, including giving it a name and description, and structuring the response using a JSON object with input schema properties (01:10:20). They illustrated various data types (e.g., string, array) that can be defined within the schema and how to use descriptions within the schema to give the LLM specific instructions, such as providing a list or adopting a specific tone like Yoda (01:11:44).
Required Parameters and Chain of Messages  Neel Seth highlighted the importance of the `required` parameter within the tool definition, noting that all fields marked as required must be included in the model's response, which is crucial for building chains of Messages  that work in a sequence. If a field is not required and the model's temperature is high, the model might skip it if it struggles to find the value, but if the value is readily available, the model will generally include it (01:14:07). The system prompt can still be used alongside tools to add further instruction (01:15:08).
Tool Use in API Response When a tool is used, the raw API response includes a specific parameter indicating "tool use" and the name of the tool, which serves as a clear signal for developers that the expected data structure is available for parsing (01:16:21). This mechanism eliminates unwanted extra sentences and guarantees the data structure, significantly increasing the accuracy of integrations (01:17:49). Neel Seth also described how tools can be used to simulate third-party API calls, where the LLM defines the required input parameters for another system (01:18:38).
Tool Use Case and Yoda Example Neel Seth showed the structured output from the defined tool, demonstrating how the LLM generated responses that adhere to the specified schema, including strings for the greeting and descriptions, and arrays (lists) for multi-item fields (01:20:28). The example also confirmed that instructions added in the tool's description, such as asking for a response "in a way Yoda would say it," were successfully executed, proving that tools are an effective, structured way of prompting for definite outputs (01:22:15).
Tools for Agent Building and Data Analysis Neel Seth confirmed that while tools primarily provide output structure, their more significant use is in building AI agents by simulating a sequence of tool calls for complex tasks, such as reading documents, analyzing data, and performing API calls (01:24:28). They illustrated an advanced scenario where an agent could execute a tool to read a Notion document and then use the output as a required parameter for a subsequent Freshdesk tool call, ensuring a logical workflow and high accuracy (01:26:41). This chaining ability allows for complex workflows involving multiple steps and external data sources, with models capable of sequencing up to 150 tools (01:29:33).
Real Business Use Case: Skill Assessment Platform Neel Seth presented a real business use case involving an online coding and interview platform (referred to as "leetcode" for example) tasked with assessing a large list of new technical skills, such as AI data modeling and security (01:50:56). The challenge for product managers was determining if these skills were assessable with their current infrastructure and what infra changes might be needed, a task that would normally take months of expert consultation and research (01:54:03).
Using Large System Prompts for Skill Analysis The proposed first-level solution involved using a large system prompt defining the LLM as a specialized skill analysis agent (01:55:06). This prompt includes the details of the platform’s current infrastructure and constraints, such as the web-based ID, use of the JDoodle API for code execution, limited execution time, and lack of file storage or database support (01:56:53). The LLM's vast knowledge base is leveraged to assess the feasibility of testing the new skills under these specific constraints (01:56:01).
Automating Skill Analysis with an LLM Script Neel Seth explained the structure of a script designed to automate the analysis of a list of skills, emphasizing that it functions as an API call script rather than a conversational chat. They noted that the script uses three variables (discipline name, mega skill, and micro skill) to make it repeatable for analyzing a large list of skills (01:58:36). The goal is to enforce a structured output, preventing the LLM from returning a simple paragraph by defining a tool called `return analysis event` (01:59:53).
Tool Definition and Output Requirements Neel Seth detailed the required outputs for the analysis tool, which include determining if a technical assessment is required (yes/no), if it can be done in the current setup (yes/no), and specific details if required, such as assessment name, process, infra requirements, and reasons for any limitations (02:00:50). They explained that variables are added to the system prompt and user message to ensure the script can run at once for all data points (02:01:45).
Script Execution and Data Handling The core of the script involves preparing the system prompt with data from a CSV, preparing the defined tool, and making the API call within a loop (02:02:41). Neel Seth mentioned that the script includes a sequence where the assistant can talk to themself to generate counter thoughts before saving the final structured results to a CSV file (02:03:51). They demonstrated running the script with a small dataset to save results in the specified format, which can be extended to other functions like pushing data to a CRM (02:04:45).
Improving Accuracy with Web Search Tool Neel Seth introduced an extension of the script (step seven) to improve accuracy and provide backup for the analysis by adding a data source through a third-party tool called `web search`. This tool allows the model to search the web, giving descriptions, briefs, and citations, which addresses concerns about hallucination (02:08:57). By using this tool and adding a `citations` column to the output, the analysis becomes multimodal and verifiable (02:10:09).
Controlling Data Sources and Analysis Evolution To further improve accuracy, Neel Seth suggested controlling the `web search` tool to only allow research from specific, trusted websites, preventing the inclusion of irrelevant data (02:12:32). They summarized the evolution of the API call from a simple message to a structured JSON output, and finally to a fundamental data analysis script capable of running multiple times for data enrichment (02:13:19). The final step involved incorporating an external third-party tool (the internet) for better data and citations (02:15:11).
Transition to Agent and Chatbot Development Neel Seth briefly discussed the transition of the core logic into a more complex structure, leading to the development of a chat-based agent or a chatbot (02:16:02). They demonstrated running a local server for a Shopify support agent, which is an app where people can interact, contrasting it with the earlier terminal-based script (02:17:05). The server hosts the application, making the API accessible to other front-end teams (02:19:24).
Authentication and Tool Integration in Chatbot Neel Seth showcased the chatbot's ability to handle complex tasks like providing order status, which requires authentication (02:21:00). The authentication is demonstrated via an email-based OTP process, structured using multiple tools and third-party services (like sending an email) to ensure the AI cannot access the OTP directly (02:21:56). They clarified that the LLM only communicates the need for an action, while the actual generation and validation of the OTP are handled by the developer's Python code using the defined tools (02:23:54) (02:30:11).
Q&A on Tools, Logging, and Future Sessions Neel Seth confirmed that the tools discussed are essentially API calls to the LLM with different system prompts, where the LLM responds with a structure that dictates further actions, such as third-party API calls (02:28:17). They recommended Python for beginners due to its readability and debugging ease, starting with simple `print` statements for logging (02:30:11) (02:32:14). Finally, Neel Seth indicated that future sessions depend on feedback and response, and they plan to share the resources, including documentation from cloud providers and the files used in the session (02:26:10) (02:33:02).

Suggested next steps

Neel Seth will clean up the repo from the raw data and share the open-sourced repo, all the notes, and meeting recordings within two to three hours after the call.
Neel Seth will take a 5-6 minute break and resume the session at 12:40 to continue with the next part on data analysis, web search, and chaining of thought of a particular tool with an actual business use case.
Neel Seth will follow up with the Tushar deep dives and open source code as part of the planned further sessions.
Neel Seth will share the documents and other things, including the recording sessions and files to a repo, with the participants after the call.

You should review Gemini's notes to make sure they're accurate. Get tips and learn how Gemini takes notes
Please provide feedback about using Gemini to take notes in a short survey.
